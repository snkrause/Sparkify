{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify notebook \"Machine Learning\" on AWS\n",
    "\n",
    "This notebook connects to the full \"Sparkify\" dataset on an S3 storage at AWS. It bases on the local evaluation of the small dataset provided by udacity. The PySpark Kernel available on the EMR-Notebook at AWS misses some of the python libraries I needed to process my data and the code for plotting the data differs from my local notebook. A very good introduction on how to adjust the code is provided by Amazon: https://aws.amazon.com/de/blogs/big-data/install-python-libraries-on-a-running-cluster-with-emr-notebooks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install missing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pandas==0.25.1\") #Install pandas version 0.25.1 \n",
    "sc.install_pypi_package(\"matplotlib==3.1.1\", \"https://pypi.org/simple\") #Install matplotlib from given PyPI repository\n",
    "sc.install_pypi_package(\"plotly\") #Install plotly\n",
    "sc.install_pypi_package(\"openpyxl\") #Install openpyxl\n",
    "sc.install_pypi_package(\"seaborn\") #Install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import all libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc, count, countDistinct\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import mean as Fmean\n",
    "from pyspark.sql.functions import round as Fround\n",
    "from pyspark.sql.functions import max as Fmax\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, Normalizer, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define metrics for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluator(results):\n",
    "    \"\"\"\n",
    "    This function calculates the true/false positive/negative prediction from the predicted test dataset.\n",
    "    Those are then used to calculate the model evaluation metrics and a confusion matrix.\n",
    "    \n",
    "    INPUT:\n",
    "    results: test-dataset including the prediction column\n",
    "    \n",
    "    OUTPUT:\n",
    "    standard metrics for model evaluation and a confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    #extract right and wrong predicted values and count their numbers \n",
    "    true_negative = results.filter((results.prediction == 0)&(results.churn==0)).count() * 1.0 \n",
    "    false_positive = results.filter((results.prediction == 1)&(results.churn==0)).count() * 1.0 \n",
    "    false_negative = results.filter((results.prediction == 0)&(results.churn==1)).count() * 1.0 \n",
    "    true_positive = results.filter((results.prediction == 1)&(results.churn==1)).count() * 1.0 \n",
    "    \n",
    "    #calculate standard measures for evaluating the model\n",
    "    accuracy = (true_positive+true_negative)/(true_negative+false_positive+false_negative+true_positive)\n",
    "    precision = true_positive/(true_positive+false_positive)\n",
    "    recall = true_positive/(true_positive+false_negative)\n",
    "    f1 = 2.0 * (precision * recall)/(precision + recall)\n",
    "    \n",
    "    #\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    print(\"F1-Score: {}\".format(f1))\n",
    "    print(\"Recall: {}\".format(recall))\n",
    "    \n",
    "    #create confusion matrix to illustrate model quality\n",
    "    print(\"\\n Confusion Matrix \\n\")\n",
    "    print(\"TRUE_NEGATIVE:{} \t FALSE_POSITIVE:{}\".format(true_negative,false_positive))\n",
    "    print(\"FALSE_NEGATIVE:{} \t TRUE_POSITIVE: {}\".format(false_negative, true_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e7289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data from S3-repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in full sparkify dataset\n",
    "event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "user_log = spark.read.json(event_data)\n",
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5fb1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop invalid (guest) userId's\n",
    "user_log = user_log.where(user_log.userId != \"\")\n",
    "print(\"dataframe w/o empty userId's has\",user_log.count(),\"rows and\",len(user_log.columns),\"columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate rows\n",
    "user_log = user_log.dropDuplicates()\n",
    "print(\"dataframe w/o duplicates has\",user_log.count(),\"rows and\",len(user_log.columns),\"columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature creation and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create new feature \"membership_days\" as mutual time reference for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc85da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log=user_log.withColumn(\"membership_days\", Fround((col('ts')/1000-col('registration')/1000)/86400).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert columns with categorical variables into 0 and 1 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list=['auth','gender','level','status','page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_add=[]\n",
    "for column in column_list:\n",
    "    categories = user_log.select(column).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    cols_add = cols_add + [F.when(F.col(column) == cat, 1).otherwise(0).alias(column + \"_\" + str(cat).lower().replace(\" \",\"_\")) for cat in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_features =user_log.select(\"userId\",\"membership_days\",*cols_add)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dataframe has\",user_log_features.count(),\"rows and\",len(user_log_features.columns),\"columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate data by \"userId\" to prepare for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=user_log_features\\\n",
    "        .groupBy(\"userId\") \\\n",
    "        .agg( \\\n",
    "             Fmax(\"membership_days\").alias(\"max_membership_days\"), \\\n",
    "             countDistinct(\"membership_days\").alias(\"active_days\"), \\\n",
    "             Fmax(\"gender_m\").alias(\"gender_m\"), \\\n",
    "             Fmax(\"level_paid\").alias(\"level_paid\"), \\\n",
    "             Fsum(\"status_200\").alias(\"sum_status_200\"), \\\n",
    "             Fsum(\"status_307\").alias(\"sum_status_307\"), \\\n",
    "             Fsum(\"status_404\").alias(\"sum_status_404\"), \\\n",
    "             Fsum(\"page_nextsong\").alias(\"sum_page_nextsong\"), \\\n",
    "             Fsum(\"page_add_to_playlist\").alias(\"sum_page_add_to_playlist\"), \\\n",
    "             Fsum(\"page_roll_advert\").alias(\"sum_page_roll_advert\"), \\\n",
    "             Fsum(\"page_thumbs_up\").alias(\"sum_page_thumbs_up\"), \\\n",
    "             Fsum(\"page_home\").alias(\"sum_page_home\"), \\\n",
    "             Fsum(\"page_logout\").alias(\"sum_page_logout\"), \\\n",
    "             Fsum(\"page_help\").alias(\"sum_page_help\"), \\\n",
    "             Fsum(\"page_upgrade\").alias(\"sum_page_upgrade\"), \\\n",
    "             Fsum(\"page_add_friend\").alias(\"sum_page_add_friend\"), \\\n",
    "             Fsum(\"page_settings\").alias(\"sum_page_settings\"), \\\n",
    "             Fsum(\"page_submit_upgrade\").alias(\"sum_page_submit_upgrade\"), \\\n",
    "             Fsum(\"page_about\").alias(\"sum_page_about\"), \\\n",
    "             Fsum(\"page_submit_downgrade\").alias(\"sum_page_submit_downgrade\"), \\\n",
    "             Fsum(\"page_error\").alias(\"sum_page_error\"), \\\n",
    "             Fsum(\"page_save_settings\").alias(\"sum_page_save_settings\"), \\\n",
    "             Fsum(\"page_cancel\").alias(\"sum_page_cancel\"), \\\n",
    "             Fsum(\"page_cancellation_confirmation\").alias(\"churn\") \\\n",
    "             ).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dataframe 'df' has\",df.count(),\"rows and\",len(df.columns),\"columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## machine learning section\n",
    "### Split In Test And Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "(training_data, test_data) = df.randomSplit([0.8,0.2], seed = 42)\n",
    "print(\"Training Dataset Count: \" + str(training_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of features\n",
    "input_cols=df.columns[1:-2]\n",
    "print('Feature overview:',input_cols)\n",
    "\n",
    "#Configure an ML pipeline, which consists of three stages: assemble, normalize, estimator\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol='features')\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"ScaledFeatures\", withMean=True, withStd=True) \n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"churn\", \n",
    "                            featuresCol=\"ScaledFeatures\")\n",
    "dt = DecisionTreeClassifier(featuresCol = \"ScaledFeatures\", \n",
    "                            labelCol = \"churn\")\n",
    "\n",
    "pipeline_rf=Pipeline(stages=[assembler, scaler, rf])\n",
    "pipeline_dt=Pipeline(stages=[assembler, scaler, dt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Decision Tree Classifier to training data and transform test data\n",
    "model_dt = pipeline_dt.fit(training_data)\n",
    "dt_predictions = model_dt.transform(test_data)\n",
    "#Determine accuracy, f1, and precision of prediction\n",
    "model_evaluator(dt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_dt = model_dt.stages[-1].featureImportances.toArray()\n",
    "feature_ranking_dt=pd.DataFrame(data={'Features': np.array(input_cols),'Importance':importances_dt})\\\n",
    "                    .sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "feature_ranking_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Random Forest Classifier to training data and transform test data\n",
    "model_rf = pipeline_rf.fit(training_data)\n",
    "rf_predictions = model_rf.transform(test_data)\n",
    "\n",
    "#Determine accuracy, f1, and precision of prediction\n",
    "model_evaluator(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank features\n",
    "importances_rf = model_rf.stages[-1].featureImportances.toArray()\n",
    "feature_ranking_rf=pd.DataFrame(data={'Features': np.array(input_cols),'Importance':importances_rf})\\\n",
    "                    .sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "feature_ranking_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Via CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - setting up parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxBins, [20, 30, 40]) \\\n",
    "    .addGrid(rf.maxDepth, [3, 4, 5, 6]) \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "    .addGrid(rf.impurity, ['gini','entropy']) \\\n",
    "    .build()\n",
    "\n",
    "crossval_rf = CrossValidator(estimator=pipeline_rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol = \"churn\", metricName = 'f1'),\n",
    "                          numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel_rf = crossval_rf.fit(training_data)\n",
    "prediction_rf = cvModel_rf.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Report Of Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show results for all model parameters\n",
    "params_rf_cv = [{p.name: v for p, v in m.items()} for m in cvModel_rf.getEstimatorParamMaps()]\n",
    "results_rf_cv=pd.DataFrame.from_dict([\n",
    "    {cvModel.getEvaluator().getMetricName(): metric, **ps} \n",
    "    for ps, metric in zip(params_rf_cv, cvModel_rf.avgMetrics)\n",
    "])\n",
    "\n",
    "results_rf_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check best model performance\n",
    "model_evaluator(prediction_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank features of best model\n",
    "importances_rf_cv = cvModel_rf.bestModel.stages[-1].featureImportances.toArray()\n",
    "feature_ranking_rf_cv=pd.DataFrame(data={'Features': np.array(input_cols),'Importance':importances_rf_cv})\\\n",
    "                    .sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "feature_ranking_rf_cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
