{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify notebook \"Data Exploration\" on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook connects to the full \"Sparkify\" dataset on an S3 storage at AWS. It bases on the local evaluation of the small dataset provided by udacity. The PySpark Kernel available on the EMR-Notebook at AWS misses some of the python libraries I needed to process my data and the code for plotting the data differs from my local notebook. A very good introduction on how to adjust the code is provided by Amazon: https://aws.amazon.com/de/blogs/big-data/install-python-libraries-on-a-running-cluster-with-emr-notebooks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install missing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pandas==0.25.1\") #Install pandas version 0.25.1 \n",
    "sc.install_pypi_package(\"matplotlib==3.1.1\", \"https://pypi.org/simple\") #Install matplotlib from given PyPI repository\n",
    "sc.install_pypi_package(\"plotly\") #Install plotly\n",
    "sc.install_pypi_package(\"openpyxl\") #Install openpyxl\n",
    "sc.install_pypi_package(\"seaborn\") #Install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import all libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import round as Fround\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e7289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data from S3-repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in full sparkify dataset\n",
    "event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "user_log = spark.read.json(event_data)\n",
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define function for aggregation by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract calendar week from ts\n",
    "spark.udf.register(\"get_week\", lambda x: str(datetime.datetime.fromtimestamp(x / 1000.0).isocalendar()[1])+\"/\"+str(datetime.datetime.fromtimestamp(x / 1000.0).isocalendar()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count all nan, Null or empty values for each row\n",
    "user_log.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in user_log.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5fb1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop invalid (guest) userId's\n",
    "user_log_valid = user_log.where(user_log.userId != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if any of the played songs have true Null values\n",
    "user_log.where((user_log.artist.isNull()) & (user_log.page == 'Next Song')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create new feature \"membership_days\" as mutual time reference for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc85da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log=user_log.withColumn(\"membership_days\", Fround((col('ts')/1000-col('registration')/1000)/86400).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create view for SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e152733",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.createOrReplaceTempView(\"user_log_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## explore column \"page\" statistic and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b209f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=user_log.columns\n",
    "temp=pd.DataFrame()\n",
    "for col in cols:\n",
    "    temp[col]=spark.sql('''\n",
    "              SELECT count(DISTINCT {0}) as {1}\n",
    "              FROM user_log_table \n",
    "              '''.format(col, col)\n",
    "              ).toPandas()[col]\n",
    "unique_values=np.transpose(temp).rename(columns={0:'unique_values'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d321d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_statistics=user_log.groupBy('page').count().sort(desc(\"count\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece73b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(12,9))\n",
    "page_statistics.plot(kind='bar', x='page',y='count', rot=30, figsize=(12,6), logy=True)\n",
    "#plt.xticks(num_of_reviews_by_year.year)\n",
    "#plt.xlim(1995, 2015)\n",
    "plt.xticks(fontsize=5)\n",
    "plt.title('statistics of page column')\n",
    "plt.xlabel('page')\n",
    "plt.ylabel('count')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## explore statistic for \"churn\" users\n",
    "### gender dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b09a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_churned=spark.sql('''\n",
    "          SELECT *, ts/1000 timestamp, get_week(ts) as week\n",
    "          FROM user_log_table \n",
    "          WHERE page == \"Cancellation Confirmation\"\n",
    "          SORT BY userId ASC\n",
    "          '''\n",
    "          )\n",
    "df_all=users_churned.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bfdefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=users_churned.groupBy(\"gender\").count().toPandas()\n",
    "plt.clf()\n",
    "df.plot(kind='bar', x='gender',y='count', rot=0, legend=True, figsize=(8,6))\n",
    "plt.title('statistics of gender')\n",
    "plt.xlabel('gender')\n",
    "plt.ylabel('count')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### level dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8777e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subscription dependency\n",
    "df2=users_churned.groupBy(\"level\").count().toPandas()\n",
    "plt.clf()\n",
    "df2.plot(kind='bar', x='level',y='count', legend=True, figsize=(8,6))\n",
    "#plt.xticks(num_of_reviews_by_year.year)\n",
    "#plt.xlim(1995, 2015)\n",
    "plt.title('statistics of level')\n",
    "plt.xlabel('level')\n",
    "plt.ylabel('count')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distribution of membership day on which service was cancelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee224322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subscription dependency\n",
    "plt.clf()\n",
    "sns.histplot(df_all, x='membership_days', discrete=True, hue='level')\n",
    "#plt.xticks(num_of_reviews_by_year.year)\n",
    "#plt.xlim(1995, 2015)\n",
    "plt.title('statistics of membership_days at service termination')\n",
    "plt.xlabel('membership_days')\n",
    "plt.ylabel('count')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statistics by week\n",
    "### user activity by level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8935bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_stats=spark.sql('''\n",
    "          SELECT COUNT(DISTINCT userId) as active_users, level, get_week(ts) as week\n",
    "          FROM user_log_table \n",
    "          GROUP BY week, level\n",
    "          '''\n",
    "          ).sort(\"week\",\"level\").toPandas()\n",
    "plt.clf()\n",
    "sns.barplot(data=week_stats, x='week', y='active_users', hue='level')\n",
    "#plt.xticks(num_of_reviews_by_year.year)\n",
    "#plt.xlim(1995, 2015)\n",
    "plt.title('weekly statistic of paid vs. free')\n",
    "plt.xlabel('week')\n",
    "plt.ylabel('active_users')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### users dropping out by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_stats_3=spark.sql('''\n",
    "          SELECT COUNT(userId) as churn_users, level, get_week(ts) as week\n",
    "          FROM user_log_table \n",
    "          WHERE page == \"Cancellation Confirmation\"\n",
    "          GROUP BY week, level\n",
    "          '''\n",
    "          ).sort(\"week\",\"level\").toPandas()\n",
    "plt.clf()\n",
    "sns.barplot(data=week_stats_3, x='week', y='churn_users', hue='level')\n",
    "#plt.xticks(num_of_reviews_by_year.year)\n",
    "#plt.xlim(1995, 2015)\n",
    "plt.title('weekly statistic of users dropping out')\n",
    "plt.xlabel('week')\n",
    "plt.ylabel('churn_users')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new users by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_stats_2=spark.sql('''\n",
    "          SELECT COUNT(userId) as new_users, get_week(ts) as week\n",
    "          FROM user_log_table \n",
    "          WHERE page == \"Submit Registration\"\n",
    "          GROUP BY week\n",
    "          '''\n",
    "          ).sort(\"week\").toPandas()\n",
    "plt.clf()\n",
    "sns.barplot(data=week_stats_2, x='week', y='new_users', palette=\"Blues_d\")\n",
    "#plt.xticks(num_of_reviews_by_year.year)\n",
    "#plt.xlim(1995, 2015)\n",
    "plt.title('weekly statistic of users registering')\n",
    "plt.xlabel('week')\n",
    "plt.ylabel('new_users')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upgrading users by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65049027",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_stats_4=spark.sql('''\n",
    "          SELECT COUNT(userId) as upgraded_users, get_week(ts) as week\n",
    "          FROM user_log_table \n",
    "          WHERE page == \"Submit Upgrade\"\n",
    "          GROUP BY week, level\n",
    "          '''\n",
    "          ).sort(\"week\",\"level\").toPandas()\n",
    "plt.clf()\n",
    "sns.barplot(data=week_stats_4, x='week', y='upgraded_users', palette=\"Blues_d\")\n",
    "#plt.xticks(num_of_reviews_by_year.year)\n",
    "#plt.xlim(1995, 2015)\n",
    "plt.title('weekly statistic of users upgrading')\n",
    "plt.xlabel('week')\n",
    "plt.ylabel('upgrading_users')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### downgrading users by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_stats_5=spark.sql('''\n",
    "          SELECT COUNT(DISTINCT userId) as downgraded_users, get_week(ts) as week\n",
    "          FROM user_log_table \n",
    "          WHERE page == \"Submit Downgrade\"\n",
    "          GROUP BY week, level\n",
    "          '''\n",
    "          ).sort(\"week\",\"level\").toPandas()\n",
    "plt.clf()\n",
    "sns.barplot(data=week_stats_5, x='week', y='downgraded_users', palette=\"Blues_d\")\n",
    "#plt.xticks(num_of_reviews_by_year.year)\n",
    "#plt.xlim(1995, 2015)\n",
    "plt.title('weekly statistic of users downgrading')\n",
    "plt.xlabel('week')\n",
    "plt.ylabel('downgrading_users')\n",
    "%matplot plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
